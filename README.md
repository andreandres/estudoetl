ğŸš€ Pipeline ETL com Python e Pandas
Este projeto Ã© um estudo prÃ¡tico de Engenharia de Dados que implementa um processo de ETL (ExtraÃ§Ã£o, TransformaÃ§Ã£o e Carga) consumindo dados da API pÃºblica DummyJSON. O objetivo Ã© demonstrar a automaÃ§Ã£o na coleta de dados brutos e a sua posterior conversÃ£o para formatos estruturados.

ğŸ—ï¸ Estrutura do Pipeline
O projeto foi dividido nas seguintes etapas lÃ³gicas:

ExtraÃ§Ã£o (extract_data): Realiza requisiÃ§Ãµes GET para os endpoints da API, tratando possÃ­veis erros de conexÃ£o.

Carga Inicial (load_data & loop_load_data): Salva os dados brutos em formato .json dentro da pasta data/, simulando uma camada de Data Lake (Raw/Bronze).

TransformaÃ§Ã£o (transform_data_json_to_csv): Utiliza a biblioteca Pandas para ler os arquivos JSON, processÃ¡-los em um DataFrame e exportÃ¡-los como arquivos .csv na pasta curated/ (camada de dados limpos/Silver).

ğŸ› ï¸ Tecnologias e Bibliotecas
Python 3.x.

Requests: Para consumo da API REST.

Pandas: Para manipulaÃ§Ã£o e transformaÃ§Ã£o de dados estruturados.

JSON: Para manipulaÃ§Ã£o de arquivos de dados semi-estruturados.

ğŸ“ OrganizaÃ§Ã£o de Pastas
Plaintext

estudoetl/
â”œâ”€â”€ data/           # Camada Raw: Arquivos .json originais da API
â”‚   â”œâ”€â”€ products/
â”‚   â””â”€â”€ user/
â”œâ”€â”€ curated/        # Camada Silver: Arquivos .csv transformados e prontos para anÃ¡lise
â”‚   â”œâ”€â”€ products/
â”‚   â””â”€â”€ user/
â”œâ”€â”€ etl.py          # Script principal com a lÃ³gica do processo
â””â”€â”€ README.md       # DocumentaÃ§Ã£o do projeto
